{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "df = pd.read_csv(cwd + \"/data_csv/preprocessing_data.csv\")\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANGER</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DISGUST</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAPPINESS</th>\n",
       "      <td>2107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEUTRAL</th>\n",
       "      <td>2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SADNESS</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SURPRISE</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image\n",
       "emotion         \n",
       "ANGER         68\n",
       "DISGUST        5\n",
       "HAPPINESS   2107\n",
       "NEUTRAL     2026\n",
       "SADNESS       34\n",
       "SURPRISE      80"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.DataFrame(train, columns=[\"image\", \"emotion\"])\n",
    "train.to_csv(cwd + \"/data_csv/train.csv\")\n",
    "\n",
    "train.groupby('emotion').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANGER</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DISGUST</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAPPINESS</th>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEUTRAL</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SADNESS</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SURPRISE</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image\n",
       "emotion         \n",
       "ANGER         10\n",
       "DISGUST        1\n",
       "HAPPINESS    252\n",
       "NEUTRAL      210\n",
       "SADNESS        2\n",
       "SURPRISE       6"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame(test, columns=[\"image\", \"emotion\"])\n",
    "test.to_csv(cwd + \"/data_csv/test.csv\")\n",
    "\n",
    "test.groupby('emotion').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport cv2,matplotlib.pyplot as plt,dlib,imutils\\nfrom imutils import face_utils\\n\\ndetector = dlib.get_frontal_face_detector()\\npredictor = dlib.shape_predictor(\"../input/dlib-68/shape_predictor_68_face_landmarks.dat\")\\n\\nimage=plt.imread(\"../input/recognizing-faces-in-the-wild/train/F0002/MID1/P00009_face3.jpg\")\\n# image = imutils.resize(image, width=500)\\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\nrects = detector(gray, 1)\\n\\nfor rect in rects:\\n    pred=predictor(gray,rect)\\n    fig, ax1 = plt.subplots()\\n\\n    ax1.imshow(image)\\n    ax1.scatter(face_utils.shape_to_np(pred)[:,0],face_utils.shape_to_np(pred)[:,1])\\n    \\n# del predictor\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import cv2,matplotlib.pyplot as plt,dlib,imutils\n",
    "from imutils import face_utils\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"../input/dlib-68/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "image=plt.imread(\"../input/recognizing-faces-in-the-wild/train/F0002/MID1/P00009_face3.jpg\")\n",
    "# image = imutils.resize(image, width=500)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "rects = detector(gray, 1)\n",
    "\n",
    "for rect in rects:\n",
    "    pred=predictor(gray,rect)\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.imshow(image)\n",
    "    ax1.scatter(face_utils.shape_to_np(pred)[:,0],face_utils.shape_to_np(pred)[:,1])\n",
    "    \n",
    "# del predictor\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.feature_selection import chi2\\n\\ndata = pd.read_csv(cwd + \"/train_data.csv\")\\n\\n#independent columns\\nX = data.iloc[:,0:20]\\n\\n#target column\\ny = data.iloc[:,-1]  \\n\\n#apply SelectKBest class to extract top 10 best features\\nbestfeatures = SelectKBest(score_func=chi2, k=10)\\nfit = bestfeatures.fit(X,y)\\ndfscores = pd.DataFrame(fit.scores_)\\ndfcolumns = pd.DataFrame(X.columns)\\n\\n#concat two dataframes for better visualization \\nfeatureScores = pd.concat([dfcolumns, dfscores],axis=1)\\n\\n#naming the dataframe columns\\nfeatureScores.columns = [\\'Specs\\',\\'Score\\']\\n\\n#print 10 best features\\nprint(featureScores.nlargest(10,\\'Score\\'))\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "data = pd.read_csv(cwd + \"/train_data.csv\")\n",
    "\n",
    "#independent columns\n",
    "X = data.iloc[:,0:20]\n",
    "\n",
    "#target column\n",
    "y = data.iloc[:,-1]  \n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns, dfscores],axis=1)\n",
    "\n",
    "#naming the dataframe columns\n",
    "featureScores.columns = ['Specs','Score']\n",
    "\n",
    "#print 10 best features\n",
    "print(featureScores.nlargest(10,'Score'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.preprocessing import StandardScaler\\n\\nlabel = train.head(1000)\\ndata = train.head(1000)\\ndata.shape\\n\\nstd = StandardScaler().fit_transform(data)\\nstd.shape\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "label = train.head(1000)\n",
    "data = train.head(1000)\n",
    "data.shape\n",
    "\n",
    "std = StandardScaler().fit_transform(data)\n",
    "std.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport cv2, glob, random, math, numpy as np, dlib, itertools\\nfrom sklearn.svm import SVC\\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\\nfrom sklearn.multiclass import OneVsRestClassifier\\nfrom sklearn import linear_model\\n\\nemotions = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]#, \"Happy\", \"Neutral\", \"Sad\"] #Emotion list\\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\\ndetector = dlib.get_frontal_face_detector()\\npredictor = dlib.shape_predictor(\"/content/drive/MyDrive/Colab Notebooks/shape_predictor_68_face_landmarks.dat\") \\n\\n#clf = SVC(kernel=\\'linear\\', probability=True, tol=1e-3)\\n\\nclf = RandomForestClassifier(min_samples_leaf=3)\\n\\n#clf = linear_model.LogisticRegression(C=1e5)\\n\\n\\n#n_estimators = 10\\n#clf = OneVsRestClassifier(BaggingClassifier(SVC(kernel=\\'linear\\', probability=True, class_weight=\\'balanced\\'), max_samples=1.0 / n_estimators, n_estimators=n_estimators),n_jobs=-1)\\n\\n\\ndef get_files(emotion):\\n    print(\"/content/drive/MyDrive/Colab Notebooks/images/%s/*\" %emotion)\\n    files = glob.glob(r\"/content/drive/MyDrive/Colab Notebooks/images/%s/*\" %emotion)\\n    print(len(files))\\n   \\n    random.shuffle(files)\\n    training = files[:int(len(files)*0.8)] #get first 80% of file list\\n    prediction = files[-int(len(files)*0.2):] #get last 20% of file list\\n   \\n    return training, prediction\\n\\ndef get_landmarks(image):\\n    detections = detector(image, 1)\\n    for k,d in enumerate(detections): #For all detected face instances individually\\n        shape = predictor(image, d) #Draw Facial Landmarks with the predictor class\\n        xlist = []\\n        ylist = []\\n        for i in range(1,68): #Store X and Y coordinates in two lists\\n            xlist.append(float(shape.part(i).x))\\n            ylist.append(float(shape.part(i).y))\\n            \\n        xmean = np.mean(xlist) #Get the mean of both axes to determine centre of gravity\\n        ymean = np.mean(ylist)\\n        xcentral = [(x-xmean) for x in xlist] #get distance between each point and the central point in both axes\\n        ycentral = [(y-ymean) for y in ylist]\\n\\n        if xlist[26] == xlist[29]: #If x-coordinates of the set are the same, the angle is 0, catch to prevent \\'divide by 0\\' error in function\\n            anglenose = 0\\n        else:\\n            anglenose = int(math.atan((ylist[26]-ylist[29])/(xlist[26]-xlist[29]))*180/math.pi)\\n\\n        if anglenose < 0:\\n            anglenose += 90\\n        else:\\n            anglenose -= 90\\n\\n        landmarks_vectorised = []\\n        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\\n            landmarks_vectorised.append(x)\\n            landmarks_vectorised.append(y)\\n            meannp = np.asarray((ymean,xmean))\\n            coornp = np.asarray((z,w))\\n            dist = np.linalg.norm(coornp-meannp)\\n            anglerelative = (math.atan((z-ymean)/(w-xmean))*180/math.pi) - anglenose\\n            landmarks_vectorised.append(dist)\\n            landmarks_vectorised.append(anglerelative)\\n\\n    if len(detections) < 1: \\n        landmarks_vectorised = \"error\"\\n    return landmarks_vectorised\\n\\ndef make_sets():\\n    training_data = []\\n    training_labels = []\\n    prediction_data = []\\n    prediction_labels = []\\n    for emotion in emotions:\\n        training, prediction = get_files(emotion)\\n        for item in training:\\n           \\n            image = cv2.imread(item) #open image\\n            if image is not None:\\n              #print(image)\\n              #gray = cv2.cvtColor(np.float32(image), cv2.COLOR_RGB2GRAY)\\n              gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\\n              #gray = rgb2gray(image)\\n\\n              clahe_image = clahe.apply(gray)\\n              landmarks_vectorised = get_gabor(clahe_image)\\n            #if landmarks_vectorised == \"error\":\\n                #pass\\n            #else:\\n              training_data.append(landmarks_vectorised) #append image array to training data list\\n              training_labels.append(emotions.index(emotion))\\n    \\n        for item in prediction:\\n         \\n            image = cv2.imread(item)\\n            if image is not None:\\n              #gray = cv2.cvtColor(np.float32(image), cv2.COLOR_RGB2GRAY)\\n              gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n              #gray = rgb2gray(image)\\n\\n              clahe_image = clahe.apply(gray)\\n              landmarks_vectorised = get_gabor(clahe_image)\\n            #if landmarks_vectorised == \"error\":\\n                #pass\\n            #else:\\n              prediction_data.append(landmarks_vectorised)\\n              prediction_labels.append(emotions.index(emotion))\\n\\n    return training_data, training_labels, prediction_data, prediction_labels   \\n\\naccur_lin = [] # 10 random set generation\\nfor i in range(0,10):\\n    print(\"Making sets %s\" %i) #Make sets by random sampling 80/20%\\n    training_data, training_labels, prediction_data, prediction_labels = make_sets()\\n\\n    npar_train = np.array(training_data) # numpy array\\n    npar_trainlabs = np.array(training_labels)\\n    \\n    print(\"training model %s\" %i) #train model\\n    clf.fit(npar_train, training_labels)\\n\\n    print(\"getting accuracies %s\" %i)\\n    npar_pred = np.array(prediction_data)\\n    pred_lin = clf.score(npar_pred, prediction_labels)\\n    print (\"Model: \", pred_lin)\\n    accur_lin.append(pred_lin) #Store accuracy in a list\\n\\nprint(\"Mean value accuracy in Model: %.3f\" %np.mean(accur_lin))\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import cv2, glob, random, math, numpy as np, dlib, itertools\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "emotions = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]#, \"Happy\", \"Neutral\", \"Sad\"] #Emotion list\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"/content/drive/MyDrive/Colab Notebooks/shape_predictor_68_face_landmarks.dat\") \n",
    "\n",
    "#clf = SVC(kernel='linear', probability=True, tol=1e-3)\n",
    "\n",
    "clf = RandomForestClassifier(min_samples_leaf=3)\n",
    "\n",
    "#clf = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "\n",
    "#n_estimators = 10\n",
    "#clf = OneVsRestClassifier(BaggingClassifier(SVC(kernel='linear', probability=True, class_weight='balanced'), max_samples=1.0 / n_estimators, n_estimators=n_estimators),n_jobs=-1)\n",
    "\n",
    "\n",
    "def get_files(emotion):\n",
    "    print(\"/content/drive/MyDrive/Colab Notebooks/images/%s/*\" %emotion)\n",
    "    files = glob.glob(r\"/content/drive/MyDrive/Colab Notebooks/images/%s/*\" %emotion)\n",
    "    print(len(files))\n",
    "   \n",
    "    random.shuffle(files)\n",
    "    training = files[:int(len(files)*0.8)] #get first 80% of file list\n",
    "    prediction = files[-int(len(files)*0.2):] #get last 20% of file list\n",
    "   \n",
    "    return training, prediction\n",
    "\n",
    "def get_landmarks(image):\n",
    "    detections = detector(image, 1)\n",
    "    for k,d in enumerate(detections): #For all detected face instances individually\n",
    "        shape = predictor(image, d) #Draw Facial Landmarks with the predictor class\n",
    "        xlist = []\n",
    "        ylist = []\n",
    "        for i in range(1,68): #Store X and Y coordinates in two lists\n",
    "            xlist.append(float(shape.part(i).x))\n",
    "            ylist.append(float(shape.part(i).y))\n",
    "            \n",
    "        xmean = np.mean(xlist) #Get the mean of both axes to determine centre of gravity\n",
    "        ymean = np.mean(ylist)\n",
    "        xcentral = [(x-xmean) for x in xlist] #get distance between each point and the central point in both axes\n",
    "        ycentral = [(y-ymean) for y in ylist]\n",
    "\n",
    "        if xlist[26] == xlist[29]: #If x-coordinates of the set are the same, the angle is 0, catch to prevent 'divide by 0' error in function\n",
    "            anglenose = 0\n",
    "        else:\n",
    "            anglenose = int(math.atan((ylist[26]-ylist[29])/(xlist[26]-xlist[29]))*180/math.pi)\n",
    "\n",
    "        if anglenose < 0:\n",
    "            anglenose += 90\n",
    "        else:\n",
    "            anglenose -= 90\n",
    "\n",
    "        landmarks_vectorised = []\n",
    "        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\n",
    "            landmarks_vectorised.append(x)\n",
    "            landmarks_vectorised.append(y)\n",
    "            meannp = np.asarray((ymean,xmean))\n",
    "            coornp = np.asarray((z,w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            anglerelative = (math.atan((z-ymean)/(w-xmean))*180/math.pi) - anglenose\n",
    "            landmarks_vectorised.append(dist)\n",
    "            landmarks_vectorised.append(anglerelative)\n",
    "\n",
    "    if len(detections) < 1: \n",
    "        landmarks_vectorised = \"error\"\n",
    "    return landmarks_vectorised\n",
    "\n",
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "    for emotion in emotions:\n",
    "        training, prediction = get_files(emotion)\n",
    "        for item in training:\n",
    "           \n",
    "            image = cv2.imread(item) #open image\n",
    "            if image is not None:\n",
    "              #print(image)\n",
    "              #gray = cv2.cvtColor(np.float32(image), cv2.COLOR_RGB2GRAY)\n",
    "              gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "              #gray = rgb2gray(image)\n",
    "\n",
    "              clahe_image = clahe.apply(gray)\n",
    "              landmarks_vectorised = get_gabor(clahe_image)\n",
    "            #if landmarks_vectorised == \"error\":\n",
    "                #pass\n",
    "            #else:\n",
    "              training_data.append(landmarks_vectorised) #append image array to training data list\n",
    "              training_labels.append(emotions.index(emotion))\n",
    "    \n",
    "        for item in prediction:\n",
    "         \n",
    "            image = cv2.imread(item)\n",
    "            if image is not None:\n",
    "              #gray = cv2.cvtColor(np.float32(image), cv2.COLOR_RGB2GRAY)\n",
    "              gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "              #gray = rgb2gray(image)\n",
    "\n",
    "              clahe_image = clahe.apply(gray)\n",
    "              landmarks_vectorised = get_gabor(clahe_image)\n",
    "            #if landmarks_vectorised == \"error\":\n",
    "                #pass\n",
    "            #else:\n",
    "              prediction_data.append(landmarks_vectorised)\n",
    "              prediction_labels.append(emotions.index(emotion))\n",
    "\n",
    "    return training_data, training_labels, prediction_data, prediction_labels   \n",
    "\n",
    "accur_lin = [] # 10 random set generation\n",
    "for i in range(0,10):\n",
    "    print(\"Making sets %s\" %i) #Make sets by random sampling 80/20%\n",
    "    training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "\n",
    "    npar_train = np.array(training_data) # numpy array\n",
    "    npar_trainlabs = np.array(training_labels)\n",
    "    \n",
    "    print(\"training model %s\" %i) #train model\n",
    "    clf.fit(npar_train, training_labels)\n",
    "\n",
    "    print(\"getting accuracies %s\" %i)\n",
    "    npar_pred = np.array(prediction_data)\n",
    "    pred_lin = clf.score(npar_pred, prediction_labels)\n",
    "    print (\"Model: \", pred_lin)\n",
    "    accur_lin.append(pred_lin) #Store accuracy in a list\n",
    "\n",
    "print(\"Mean value accuracy in Model: %.3f\" %np.mean(accur_lin))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train\n",
    "dictionary = {'emotion':{'HAPPINESS':0,'NEUTRAL':1, 'SURPRISE':2, 'ANGER':3, 'SADNESS':4, 'DISGUST':5}}\n",
    "df.replace(dictionary, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>Jean-David_Levitte_0009.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>Donald_Rumsfeld_0057.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Ari_Fleischer_0007.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>George_Ryan_0003.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>Costas_Simitis_0005.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>JK_Rowling_0002.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>Catherine_Zeta-Jones_0005.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4053</th>\n",
       "      <td>Salma_Hayek_0011.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>Larry_Thompson_0004.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>Nia_Vardalos_0001.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4320 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image  emotion\n",
       "2126    Jean-David_Levitte_0009.jpg        1\n",
       "1135       Donald_Rumsfeld_0057.jpg        1\n",
       "343          Ari_Fleischer_0007.jpg        1\n",
       "1439           George_Ryan_0003.jpg        1\n",
       "931         Costas_Simitis_0005.jpg        1\n",
       "...                             ...      ...\n",
       "2305            JK_Rowling_0002.jpg        0\n",
       "686   Catherine_Zeta-Jones_0005.jpg        0\n",
       "4053           Salma_Hayek_0011.jpg        1\n",
       "2776        Larry_Thompson_0004.jpg        1\n",
       "3430          Nia_Vardalos_0001.jpg        0\n",
       "\n",
       "[4320 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'part'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [81]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m ylist \u001b[39m=\u001b[39m []\n\u001b[0;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m68\u001b[39m):\n\u001b[1;32m---> 44\u001b[0m     xlist\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(shape\u001b[39m.\u001b[39;49mpart(i)\u001b[39m.\u001b[39mx))\n\u001b[0;32m     45\u001b[0m     ylist\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(shape\u001b[39m.\u001b[39mpart(i)\u001b[39m.\u001b[39my))\n\u001b[0;32m     47\u001b[0m \u001b[39m# get mean of both axes to determine centre of gravity\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'part'"
     ]
    }
   ],
   "source": [
    "import cv2, glob, random, math, numpy as np, dlib, itertools\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from imutils import face_utils\n",
    "\n",
    "# emotion list\n",
    "emotions = [0, 1, 2, 3, 4, 5, 6] \n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(cwd + \"/predictor/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "error = 0\n",
    "\n",
    "# run each image in train\n",
    "for idx, row in df.iterrows():\n",
    "    imagePath = cwd + \"/cleaned_images/\" + row.image\n",
    "    image = cv2.imread(imagePath)\n",
    "\n",
    "    rects = detector(image, 0)\n",
    "\n",
    "    if len(rects) == 0:\n",
    "        error += 1\n",
    "        print(row.image)\n",
    "    \n",
    "    # loop over the face detections\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        shape = predictor(image, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "    \n",
    "        # loop over the (x, y)-coordinates for the facial landmarks\n",
    "        # and draw them on the image\n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "        # store x and y coordinates in two lists\n",
    "        xlist = []\n",
    "        ylist = []\n",
    "        for i in range(1,68):\n",
    "            xlist.append(float(shape.part(i).x))\n",
    "            ylist.append(float(shape.part(i).y))\n",
    "\n",
    "        # get mean of both axes to determine centre of gravity\n",
    "        xmean = np.mean(xlist) \n",
    "        ymean = np.mean(ylist)\n",
    "\n",
    "        print(xmean, ymean)\n",
    "        \n",
    "        '''\n",
    "        # get distance between each point and the central point\n",
    "        xcentral = [(x - xmean) for x in xlist] \n",
    "        ycentral = [(y - ymean) for y in ylist]\n",
    "        '''\n",
    "            \n",
    "    # show the output image with the face detections + facial landmarks\n",
    "    cv2.imshow(\"Output\", image)\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 68:\n",
    "        break\n",
    "\n",
    "print(error)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d02cdae7faed44485a20a1dfe89a6c1f0576f684e5ae49a964539fc51d957e1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
