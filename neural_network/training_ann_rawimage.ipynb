{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import pandas as pd, numpy as np\n",
    "import cv2, imutils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from hyperopt.pyll.base import scope \n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "pre_csv = os.path.abspath(os.path.join(os.sep, cwd, '..', 'neural_network', 'augmentation_data.csv'))\n",
    "df = pd.read_csv(pre_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFullImagePath(full_path):\n",
    "    sub_image_path = full_path\n",
    "    split_sub_image_path = sub_image_path.split(\"/\")\n",
    "    image_path = \"\"\n",
    "    if(len(split_sub_image_path) == 2):\n",
    "        image_path = os.path.abspath(os.path.join(os.sep, cwd, \"..\", \"neural_network\", \"augmentation\", split_sub_image_path[0], split_sub_image_path[1]))\n",
    "    else:\n",
    "        image_path = os.path.abspath(os.path.join(os.sep, cwd, \"..\", \"neural_network\", \"augmentation\", sub_image_path))\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10000/77205\n"
     ]
    }
   ],
   "source": [
    "count = [0 for x in range(8)]\n",
    "raw_images =  []\n",
    "labels = []\n",
    "for i, row in df.iterrows():\n",
    "    image_path = getFullImagePath(row.image)\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (112, 112), interpolation = cv2.INTER_AREA)\n",
    "    pixels = image.flatten()\n",
    "    raw_images.append(pixels)\n",
    "    \n",
    "    label = row.emotion\n",
    "    labels.append(label)\n",
    "    \n",
    "    count[row.emotion] += 1\n",
    "    if i > 0 and i % 10000 == 0: print('[INFO] processed {}/{}'.format(i, len(df)))\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'rate'       : hp.uniform('rate', 0.01, 0.5),\n",
    "    'dropout'    : hp.uniform('dropout', 0.01, 0.5),\n",
    "    'units1'      : scope.int(hp.quniform('units1', 10, 100, 5)),\n",
    "    'units2'      : scope.int(hp.quniform('units2', 10, 100, 5)),\n",
    "    'units3'      : scope.int(hp.quniform('units3', 10, 100, 5)),\n",
    "    'units4'      : scope.int(hp.quniform('units4', 10, 100, 5)),\n",
    "    'batch_size' : scope.int(hp.quniform('batch_size', 100, 250, 25)),\n",
    "    'layers'     : scope.int(hp.quniform('layers', 3, 4, 1)),\n",
    "    'optimizer'  : hp.choice('optimizer', ['adam', 'adadelta', 'sgd', 'RMSprop']),\n",
    "    'epochs'     : scope.int(hp.quniform('epochs', 100, 300, 10)),\n",
    "    'activation' : hp.choice('activation', ['relu']), # ['relu', 'sigmoid', 'tanh', 'elu']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_nn(params):\n",
    "    print(\"params\", params)\n",
    "\n",
    "    # Keras LSTM model\n",
    "    model = Sequential()\n",
    "\n",
    "    # params['layers'] => layer of model (do not count input layer)\n",
    "\n",
    "    if params['layers'] == 1:\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "    else:\n",
    "        # First layer specifies input_shape and returns sequences\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "        # Middle layers return sequences\n",
    "        for i in range(params['layers']-2):\n",
    "            model.add(Dense(params['units' + str(i + 2)], activation=params['activation']))\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "            #model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "        # Last layer doesn't return anything                                            \n",
    "        model.add(Dense(params['units' + str(params['layers'])], activation=params['activation']))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    model.compile(optimizer=params['optimizer'], loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss',mode='min', verbose=1,patience=15)\n",
    "    result =  model.fit(X_train, y_train, validation_data=(X_val, y_val,), batch_size=params['batch_size'], epochs=int(X_train.shape[0]/best_params['batch_size']), verbose=0, callbacks=[es])\n",
    "\n",
    "    # Get the lowest validation loss of the training epochs\n",
    "    validation_loss = np.amin(result.history['val_loss']) \n",
    "    print('Best validation loss of epoch:', validation_loss)\n",
    "\n",
    "    return {'loss': validation_loss, \n",
    "            'status': STATUS_OK, \n",
    "            'model': model, \n",
    "            'params': params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(raw_images), np.array(labels), test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "y_val = le.transform(y_val)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "trials = Trials()\n",
    "best = fmin(f_nn, \n",
    "            space, \n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,\n",
    "            trials=trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trials.results[np.argmin([r['loss'] for r in trials.results])]['model']\n",
    "best_params = trials.results[np.argmin([r['loss'] for r in trials.results])]['params']\n",
    "worst_model = trials.results[np.argmax([r['loss'] for r in trials.results])]['model']\n",
    "worst_params = trials.results[np.argmax([r['loss'] for r in trials.results])]['params']\n",
    "\n",
    "print(best_model)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_best_param(params, output):\n",
    "    model = Sequential()\n",
    "\n",
    "    if params['layers'] == 1:\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "    else:\n",
    "        # First layer specifies input_shape and returns sequences\n",
    "        model.add(Dense(params['units1'], activation=params['activation_1'], input_shape=(X_train.shape[1],)))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "        # Middle layers return sequences\n",
    "        for i in range(params['layers']-2):\n",
    "            model.add(Dense(params['units' + str(i + 2)], activation=params['activation']))\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "            #model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "        # Last layer doesn't return anything                                            \n",
    "        model.add(Dense(params['units' + str(params['layers'])], activation=params['activation']))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "    model.add(Dense(output, activation='softmax'))\n",
    "    model.compile(optimizer=params['optimizer'], loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without augmentation\n",
    "# best_params = {'activation': 'relu', 'batch_size': 125, 'dropout': 0.022269352793562694, 'epochs': 190, 'layers': 4, 'optimizer': 'adadelta', 'rate': 0.341066733379372, 'units1': 70, 'units2': 55, 'units3': 40, 'units4': 25}\n",
    "\n",
    "# with augmentation\n",
    "# best_params = {'activation': 'relu', 'batch_size': 225, 'dropout': 0.43494195542218583, 'epochs': 210, 'layers': 3, 'optimizer': 'adadelta', 'rate': 0.3152384932786021, 'units1': 100, 'units2': 100, 'units3': 45, 'units4': 80}\n",
    "\n",
    "# flook model 0.6695\n",
    "best_params = {'activation_1': 'elu','activation': 'relu', 'batch_size': 225, 'dropout': 0.01854633556762575, 'epochs': 300, 'layers': 4, 'optimizer': 'adagrad', 'rate': 0.012361452297880514, 'units1': 100, 'units2': 70, 'units3': 100, 'units4': 100}\n",
    "\n",
    "model = create_model_best_param(best_params, 8)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val,), batch_size=best_params['batch_size'], epochs=int(X_train.shape[0]/best_params['batch_size']), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'y', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "dictionary = {0: 'ANGER', 1: 'CONTEMPT', 2: 'DISGUST', 3: 'FEAR', 4: 'HAPPINESS',  5: 'NEUTRAL', 6: 'SADNESS', 7: 'SURPRISE'}\n",
    "target_name = [dictionary[i] for i in range(len(dictionary))]\n",
    "\n",
    "value = model.predict(X_test)\n",
    "y_pred = np.argmax(value, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_true,y_pred, target_names=target_name, digits=4))\n",
    "\n",
    "print(y_test.shape, y_true.shape)\n",
    "\n",
    "dictionary = ['ANGER', 'CONTEMPT', 'DISGUST', 'FEAR', 'HAPPINESS',  'NEUTRAL', 'SADNESS', 'SURPRISE']\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dictionary)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "disp.plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "dictionary = ['ANGER', 'CONTEMPT', 'DISGUST', 'FEAR', 'HAPPINESS',  'NEUTRAL', 'SADNESS', 'SURPRISE']\n",
    "\n",
    "for idx in range(len(dictionary)):\n",
    "    print(dictionary[idx])\n",
    "\n",
    "    emo_feature = np.copy(X_train)\n",
    "    emo_target = list(label[idx] for label in y_train)\n",
    "    emo_target = np.array(emo_target)\n",
    "\n",
    "    X = emo_feature\n",
    "    y = emo_target\n",
    "\n",
    "    history = []\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # plot ROC curves\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    for i, (train, val) in enumerate(cv.split(X, y)):\n",
    "        X_train_kf, X_val_kf = X[train], X[val]\n",
    "        y_train_kf, y_val_kf = y[train], y[val]\n",
    "\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        y_train_kf = to_categorical(y_train_kf)\n",
    "        y_val_kf = to_categorical(y_val_kf)\n",
    "\n",
    "        model = create_model_best_param(best_params, 2)\n",
    "        model.fit(X_train_kf,\n",
    "                    y_train_kf,\n",
    "                    validation_data=(X_val_kf, y_val_kf,),\n",
    "                    batch_size=best_params['batch_size'],\n",
    "                    epochs=best_params[\"epochs\"],\n",
    "                    verbose=2)\n",
    "\n",
    "        # predict\n",
    "        y_pred = model.predict(X_val_kf).ravel()\n",
    "        y_val_kf = y_val_kf.ravel()\n",
    "\n",
    "        print('====================Fold ', i , '====================')\n",
    "\n",
    "        # plot ROC curve\n",
    "        viz = RocCurveDisplay.from_predictions(y_val_kf, y_pred, ax=ax, name=\"ROC fold {}\".format(i), alpha=0.3, lw=1,)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    # middle line\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "    # mean line\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    # std\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(\n",
    "        mean_fpr,\n",
    "        tprs_lower,\n",
    "        tprs_upper,\n",
    "        color=\"grey\",\n",
    "        alpha=0.2,\n",
    "        label=r\"$\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05],\n",
    "            ylim=[-0.05, 1.05],\n",
    "            title=\"Receiver operating characteristic\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.savefig(cwd + '/../graph/' + dictionary[idx] + '/ann_relu.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'model_relu_best'\n",
    "path = cwd + \"/model/\" + name\n",
    "if os.path.exists(path):\n",
    "    shutil.rmtree(path)\n",
    "os.makedirs(path)\n",
    "\n",
    "model.save(path)\n",
    "# model = keras.models.load_model(path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efc329eb76fa8c61d98fc33828834e5a32dbc6c00e0ecf46855e3ac0d7163765"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
